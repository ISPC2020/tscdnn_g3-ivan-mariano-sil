import numpy as np

class Metodos:

    # Initialize parameters    
    
    def initialize_parameters(layers_dims):
        """
        Initialize parameters dictionary.
    
        Weight matrices will be initialized to random values from uniform normal
        distribution.
        bias vectors will be initialized to zeros.

        Arguments
        ---------
        layers_dims : list or array-like
            dimensions of each layer in the network.

        Returns
        -------
        parameters : dict
            weight matrix and the bias vector for each layer.
        """
        np.random.seed(1)               
        parameters = {}
        L = len(layers_dims)            

        for l in range(1, L):           
            parameters["W" + str(l)] = np.random.randn(
                layers_dims[l], layers_dims[l - 1]) * 0.01
            parameters["b" + str(l)] = np.zeros((layers_dims[l], 1))

            assert parameters["W" + str(l)].shape == (
                layers_dims[l], layers_dims[l - 1])
            assert parameters["b" + str(l)].shape == (layers_dims[l], 1)

        return parameters
    
    # Celda 3
    # Define activation functions that will be used in forward propagation
    def sigmoid(Z):
        """
        Computes the sigmoid of Z element-wise.

        Arguments
        ---------
        Z : array
            output of affine transformation.

        Returns
        -------
        A : array
            post activation output.
        Z : array
            output of affine transformation.
        """
        A = 1 / (1 + np.exp(-Z))

        return A, Z

    def tanh(Z):
        """
        Computes the Hyperbolic Tagent of Z elemnet-wise.

        Arguments
        ---------
        Z : array
            output of affine transformation.

        Returns
        -------
        A : array
            post activation output.
        Z : array
            output of affine transformation.
        """
        A = np.tanh(Z)

        return A, Z

    def relu(Z):
        """
        Computes the Rectified Linear Unit (ReLU) element-wise.

        Arguments
        ---------
        Z : array
            output of affine transformation.

        Returns
        -------
        A : array
            post activation output.
        Z : array
            output of affine transformation.
        """
        A = np.maximum(0, Z)

        return A, Z

    def leaky_relu(Z):
        """
        Computes Leaky Rectified Linear Unit element-wise.

        Arguments
        ---------
        Z : array
            output of affine transformation.

        Returns
        -------
        A : array
            post activation output.
        Z : array
            output of affine transformation.
        """
        A = np.maximum(0.1 * Z, Z)

        return A, Z

